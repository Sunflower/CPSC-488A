{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/sentences.txt', 'w', encoding='utf-8') as f:\n",
    "#     for sentence in sentences['sentence']:\n",
    "#         f.write('{}\\n'.format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>word_type</th>\n",
       "      <th>function</th>\n",
       "      <th>seg_type</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_tag</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25070</td>\n",
       "      <td>HER</td>\n",
       "      <td>she</td>\n",
       "      <td>DPS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25071</td>\n",
       "      <td>DRESS</td>\n",
       "      <td>dress</td>\n",
       "      <td>NN1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25072</td>\n",
       "      <td>HANGS</td>\n",
       "      <td>hang</td>\n",
       "      <td>VVZ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25073</td>\n",
       "      <td>HERE'</td>\n",
       "      <td>here'</td>\n",
       "      <td>NP0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25074</td>\n",
       "      <td>DE-FROCKING</td>\n",
       "      <td>de-frock</td>\n",
       "      <td>VVG</td>\n",
       "      <td>mrw</td>\n",
       "      <td>met</td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id         word     lemma word_type function seg_type  sentence_id  \\\n",
       "0  25070    HER          she       DPS                         1267          \n",
       "1  25071    DRESS        dress     NN1                         1267          \n",
       "2  25072    HANGS        hang      VVZ                         1267          \n",
       "3  25073    HERE'        here'     NP0                         1267          \n",
       "4  25074    DE-FROCKING  de-frock  VVG       mrw      met      1267          \n",
       "\n",
       "          text_id text_tag     genre  \n",
       "0  a6u-fragment02  a6u      academic  \n",
       "1  a6u-fragment02  a6u      academic  \n",
       "2  a6u-fragment02  a6u      academic  \n",
       "3  a6u-fragment02  a6u      academic  \n",
       "4  a6u-fragment02  a6u      academic  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>word_type</th>\n",
       "      <th>function</th>\n",
       "      <th>seg_type</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_tag</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38263</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>AT0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1947</td>\n",
       "      <td>ab9-fragment03</td>\n",
       "      <td>ab9</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38264</td>\n",
       "      <td>boy</td>\n",
       "      <td>boy</td>\n",
       "      <td>NN1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1947</td>\n",
       "      <td>ab9-fragment03</td>\n",
       "      <td>ab9</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38265</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CJC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1947</td>\n",
       "      <td>ab9-fragment03</td>\n",
       "      <td>ab9</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38266</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>AT0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1947</td>\n",
       "      <td>ab9-fragment03</td>\n",
       "      <td>ab9</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38267</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>NN1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1947</td>\n",
       "      <td>ab9-fragment03</td>\n",
       "      <td>ab9</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id word lemma word_type function seg_type  sentence_id  \\\n",
       "0  38263    The  the   AT0                         1947          \n",
       "1  38264    boy  boy   NN1                         1947          \n",
       "2  38265    and  and   CJC                         1947          \n",
       "3  38266    the  the   AT0                         1947          \n",
       "4  38267    dog  dog   NN1                         1947          \n",
       "\n",
       "          text_id text_tag    genre  \n",
       "0  ab9-fragment03  ab9      fiction  \n",
       "1  ab9-fragment03  ab9      fiction  \n",
       "2  ab9-fragment03  ab9      fiction  \n",
       "3  ab9-fragment03  ab9      fiction  \n",
       "4  ab9-fragment03  ab9      fiction  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>word_type</th>\n",
       "      <th>function</th>\n",
       "      <th>seg_type</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_tag</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Latest</td>\n",
       "      <td>late</td>\n",
       "      <td>AJS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>a1e-fragment01</td>\n",
       "      <td>a1e</td>\n",
       "      <td>newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>corporate</td>\n",
       "      <td>corporate</td>\n",
       "      <td>AJ0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>a1e-fragment01</td>\n",
       "      <td>a1e</td>\n",
       "      <td>newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>unbundler</td>\n",
       "      <td>unbundler</td>\n",
       "      <td>NN1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>a1e-fragment01</td>\n",
       "      <td>a1e</td>\n",
       "      <td>newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>reveals</td>\n",
       "      <td>reveal</td>\n",
       "      <td>VVZ</td>\n",
       "      <td>mrw</td>\n",
       "      <td>met</td>\n",
       "      <td>0</td>\n",
       "      <td>a1e-fragment01</td>\n",
       "      <td>a1e</td>\n",
       "      <td>newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>laid-back</td>\n",
       "      <td>laid-back</td>\n",
       "      <td>AJ0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>a1e-fragment01</td>\n",
       "      <td>a1e</td>\n",
       "      <td>newspapers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id       word      lemma word_type function seg_type  sentence_id  \\\n",
       "0  0        Latest     late       AJS                         0             \n",
       "1  1        corporate  corporate  AJ0                         0             \n",
       "2  2        unbundler  unbundler  NN1                         0             \n",
       "3  3        reveals    reveal     VVZ       mrw      met      0             \n",
       "4  4        laid-back  laid-back  AJ0                         0             \n",
       "\n",
       "          text_id text_tag       genre  \n",
       "0  a1e-fragment01  a1e      newspapers  \n",
       "1  a1e-fragment01  a1e      newspapers  \n",
       "2  a1e-fragment01  a1e      newspapers  \n",
       "3  a1e-fragment01  a1e      newspapers  \n",
       "4  a1e-fragment01  a1e      newspapers  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>word_type</th>\n",
       "      <th>function</th>\n",
       "      <th>seg_type</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_tag</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156256</td>\n",
       "      <td>Ours</td>\n",
       "      <td>ours</td>\n",
       "      <td>PNP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8721</td>\n",
       "      <td>kb7-fragment10</td>\n",
       "      <td>kb7</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156257</td>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>VBZ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8721</td>\n",
       "      <td>kb7-fragment10</td>\n",
       "      <td>kb7</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156258</td>\n",
       "      <td>actually</td>\n",
       "      <td>actually</td>\n",
       "      <td>AV0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8721</td>\n",
       "      <td>kb7-fragment10</td>\n",
       "      <td>kb7</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156259</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>AT0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8721</td>\n",
       "      <td>kb7-fragment10</td>\n",
       "      <td>kb7</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156260</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "      <td>AJ0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8721</td>\n",
       "      <td>kb7-fragment10</td>\n",
       "      <td>kb7</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id      word     lemma word_type function seg_type  sentence_id  \\\n",
       "0  156256   Ours      ours      PNP                         8721          \n",
       "1  156257   is        be        VBZ                         8721          \n",
       "2  156258   actually  actually  AV0                         8721          \n",
       "3  156259   a         a         AT0                         8721          \n",
       "4  156260   new       new       AJ0                         8721          \n",
       "\n",
       "          text_id text_tag         genre  \n",
       "0  kb7-fragment10  kb7      conversation  \n",
       "1  kb7-fragment10  kb7      conversation  \n",
       "2  kb7-fragment10  kb7      conversation  \n",
       "3  kb7-fragment10  kb7      conversation  \n",
       "4  kb7-fragment10  kb7      conversation  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = []\n",
    "for genre in ['academic', 'fiction', 'newspapers', 'conversation']:\n",
    "    dataset = pd.read_csv('data/train/{}/words.csv'.format(genre),\n",
    "                          encoding='ISO--8859-1', na_filter=False)\n",
    "    display(dataset.head())\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164303"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat(datasets)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6266</th>\n",
       "      <td>6266</td>\n",
       "      <td>1933</td>\n",
       "      <td>See, for example, Fig. 5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>2874</td>\n",
       "      <td>942</td>\n",
       "      <td>Unemployment was 80 per cent on some streets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14369</th>\n",
       "      <td>14369</td>\n",
       "      <td>-1</td>\n",
       "      <td>You can use your cloth daddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>5631</td>\n",
       "      <td>1747</td>\n",
       "      <td>It was a dramatic and impractical outfit of a kind I only expected to see on the male models who posed in the more outlandish fashion magazines that our rich clients brought aboard, yet Jesse Isambard Sweetman managed to wear the elaborate style with an elegant insouciance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7702</th>\n",
       "      <td>7702</td>\n",
       "      <td>2299</td>\n",
       "      <td>Nevertheless a few qualitative conclusions may be drawn without doing any further mathematics.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id  paragraph_id  \\\n",
       "6266   6266         1933           \n",
       "2874   2874         942            \n",
       "14369  14369       -1              \n",
       "5631   5631         1747           \n",
       "7702   7702         2299           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 sentence  \n",
       "6266   See, for example, Fig. 5.3                                                                                                                                                                                                                                                          \n",
       "2874   Unemployment was 80 per cent on some streets.                                                                                                                                                                                                                                       \n",
       "14369  You can use your cloth daddy                                                                                                                                                                                                                                                        \n",
       "5631   It was a dramatic and impractical outfit of a kind I only expected to see on the male models who posed in the more outlandish fashion magazines that our rich clients brought aboard, yet Jesse Isambard Sweetman managed to wear the elaborate style with an elegant insouciance.  \n",
       "7702   Nevertheless a few qualitative conclusions may be drawn without doing any further mathematics.                                                                                                                                                                                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16180</th>\n",
       "      <td>16180</td>\n",
       "      <td>-1</td>\n",
       "      <td>Some else's sitting at their desk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16181</th>\n",
       "      <td>16181</td>\n",
       "      <td>-1</td>\n",
       "      <td>Well not you know cleaning so I do n't know what kind of work they do.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16182</th>\n",
       "      <td>16182</td>\n",
       "      <td>-1</td>\n",
       "      <td>Oh well if you 're here that 's all right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id  paragraph_id  \\\n",
       "16180  16180       -1              \n",
       "16181  16181       -1              \n",
       "16182  16182       -1              \n",
       "\n",
       "                                                                     sentence  \n",
       "16180  Some else's sitting at their desk?                                      \n",
       "16181  Well not you know cleaning so I do n't know what kind of work they do.  \n",
       "16182  Oh well if you 're here that 's all right                               "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences = pd.read_csv('data/raw/sentences.csv', encoding='ISO-8859-1')\n",
    "display(len(all_sentences))\n",
    "display(all_sentences.sample(5))\n",
    "all_sentences.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13134"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149369</th>\n",
       "      <td>13261</td>\n",
       "      <td>-1</td>\n",
       "      <td>Nobody mentioned that they were puppets themselves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40504</th>\n",
       "      <td>2565</td>\n",
       "      <td>847</td>\n",
       "      <td>In its place would be a sentencing discretion, which judges would use, in many cases, to mark the relative heinousness of the murder by a determinate prison sentence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154648</th>\n",
       "      <td>14452</td>\n",
       "      <td>-1</td>\n",
       "      <td>You 're gon na help me, you 're gon na stay here I 'm afraid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151522</th>\n",
       "      <td>13670</td>\n",
       "      <td>-1</td>\n",
       "      <td>Do you want a straw mum?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163506</th>\n",
       "      <td>16035</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dear me!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  paragraph_id  \\\n",
       "149369  13261       -1              \n",
       "40504   2565         847            \n",
       "154648  14452       -1              \n",
       "151522  13670       -1              \n",
       "163506  16035       -1              \n",
       "\n",
       "                                                                                                                                                                      sentence  \n",
       "149369  Nobody mentioned that they were puppets themselves.                                                                                                                     \n",
       "40504   In its place would be a sentencing discretion, which judges would use, in many cases, to mark the relative heinousness of the murder by a determinate prison sentence.  \n",
       "154648  You 're gon na help me, you 're gon na stay here I 'm afraid                                                                                                            \n",
       "151522  Do you want a straw mum?                                                                                                                                                \n",
       "163506  Dear me!                                                                                                                                                                "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences the training set contains.\n",
    "sentences = all_sentences.merge(dataset)[['sentence_id', 'paragraph_id', 'sentence']].drop_duplicates()\n",
    "display(sentences['sentence_id'].nunique())\n",
    "sentences.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6561"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95548</th>\n",
       "      <td>6241</td>\n",
       "      <td>1928</td>\n",
       "      <td>The reader is required to follow a series of steps which might, for example, introduce a change in the performance of a complex system.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115819</th>\n",
       "      <td>8059</td>\n",
       "      <td>2390</td>\n",
       "      <td>There is a potty for emergencies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47922</th>\n",
       "      <td>3027</td>\n",
       "      <td>988</td>\n",
       "      <td>But Stewart snaps: I do n't want to be disappointed so I 'm not going to ask for their help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13559</th>\n",
       "      <td>774</td>\n",
       "      <td>257</td>\n",
       "      <td>It does n't matter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110072</th>\n",
       "      <td>7481</td>\n",
       "      <td>2245</td>\n",
       "      <td> A white man has blood, red blood also.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  paragraph_id  \\\n",
       "95548   6241         1928           \n",
       "115819  8059         2390           \n",
       "47922   3027         988            \n",
       "13559   774          257            \n",
       "110072  7481         2245           \n",
       "\n",
       "                                                                                                                                       sentence  \n",
       "95548   The reader is required to follow a series of steps which might, for example, introduce a change in the performance of a complex system.  \n",
       "115819  There is a potty for emergencies.                                                                                                        \n",
       "47922   But Stewart snaps: I do n't want to be disappointed so I 'm not going to ask for their help.                                           \n",
       "13559   It does n't matter.                                                                                                                      \n",
       "110072   A white man has blood, red blood also.                                                                                                "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_sentences = sentences[sentences['paragraph_id'] != -1]\n",
    "display(len(paragraph_sentences))  # Number of sentences that belong to paragraphs.\n",
    "paragraph_sentences.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2067"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "paragraph_id\n",
       "3    IT SEEMS that Roland Franklin, the latest unbundler to appear in the UK, has made a fatal error in the preparation of his £697m break-up bid for stationery and packaging group DRG. He has not properly investigated the target's dining facilities.                                                                                                                   \n",
       "4    The 63-year-old head of Pembridge Investments, through which the bid is being mounted says, rule number one in this business is: the more luxurious the luncheon rooms at headquarters, the more inefficient the business. If he had taken his own rule seriously, he would have found out that DRG has a very modest self-service canteen at its Bristol head office.\n",
       "5    There are other things he has, on his own admission, not fully investigated, like the value of the DRG properties, or which part of the DRG business he would keep after the break up. When the bid was launched last week, Mr Franklin faced some criticism from City commentators on both those counts. He regards the charges as unfounded.                          \n",
       "6    On property, he is blunt.  I do not regard property profits as earnings. We have made a bid of nearly £700m for a company with a book value of £200m we 've acknowledged there is some extra worth there, but I can not see the sort of value they are talking about in property. If they can prove it is there, we might pay for it,                                \n",
       "7    On the other criticism he is equally dismissive.  That point about the core business is very unfair, he says. It is difficult to decide when you are an outsider what the business is like. We would eventually like to do it along with the management, within the philosophy we will impose.                                                                       \n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = paragraph_sentences.groupby('paragraph_id')['sentence'].agg(list).apply(lambda x: ' '.join(x))\n",
    "display(len(paragraphs))\n",
    "paragraphs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves all the paragraphs we created from our sentences.\n",
    "```python\n",
    "!rm data/train/paragraphs/*\n",
    "for i, paragraph in paragraphs.iteritems():\n",
    "    file_name = \"data/train/paragraphs/paragraph_{}.txt\".format(i)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        print('Writing {}'.format(file_name))\n",
    "        f.write(paragraph)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the following.\n",
    "```bash\n",
    "python scripts/paragraph_chain.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Longest Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2067"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>longest_chain</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Loading the Index... Loading the Text... Loading the Categories... holding businesses matter consequence genius concept company company</td>\n",
       "      <td>What he has learned from Goldsmith( the only genius I have ever come across) is that the holding company is not the most important unit of corporate organisation.  The concept of the company does not interest me- it 's the businesses that are important. It does not matter whether or not DRG makes sellotape or Basildon Bond it 's of no consequence to anybody if somebody else makes them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>Loading the Index... Loading the Text... Loading the Categories... ran found crop unqualified lost obliged rest money cattle</td>\n",
       "      <td>Nor could the vaunted irrigation scheme be  described as an unqualified success. The money ran out before the scheme's completion, and the villagers found themselves obliged to borrow the rest. They begged Rytasha for reimbursement, but none was forthcoming, and for the want of a few hundred pounds the village lost its crop for the year.  Now what are we supposed to do? asked one of them.  Are we to sell our cattle or our land?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>Loading the Index... Loading the Text... Loading the Categories... spiritual gurus teach form speak language</td>\n",
       "      <td>The most irritating thing about Rytasha and there were many was that, though she does not speak their language, she presumes to teach them a spurious form of their own religion.  One thing we do n't need in this subcontinent, Professor Rokeya Kabeer observed, is spiritual guidance we have gurus a dime a dozen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>Loading the Index... Loading the Text... Loading the Categories... lines white</td>\n",
       "      <td>They are far too civilised for that sort of thing in Bangladesh, but I kept thinking wistfully of T. S. Eliot's lines in Sweeney Agonistes about a nice little white little missionary stew.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>Loading the Index... Loading the Text... Loading the Categories... stock market index investors holders market surge offer risen</td>\n",
       "      <td>AT LEAST last Friday's post-election stock market surge was not a one-day special offer for the professionals only. Prices have remained the FT-SE index has risen another 55 points since then allowing even the most passive of private investors, including unit trust holders, to take advantage of the market.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph_id  \\\n",
       "0  10             \n",
       "1  1000           \n",
       "2  1001           \n",
       "3  1002           \n",
       "4  1003           \n",
       "\n",
       "                                                                                                                             longest_chain  \\\n",
       "0  Loading the Index... Loading the Text... Loading the Categories... holding businesses matter consequence genius concept company company   \n",
       "1  Loading the Index... Loading the Text... Loading the Categories... ran found crop unqualified lost obliged rest money cattle              \n",
       "2  Loading the Index... Loading the Text... Loading the Categories... spiritual gurus teach form speak language                              \n",
       "3  Loading the Index... Loading the Text... Loading the Categories... lines white                                                            \n",
       "4  Loading the Index... Loading the Text... Loading the Categories... stock market index investors holders market surge offer risen          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                              sentence  \n",
       "0  What he has learned from Goldsmith( the only genius I have ever come across) is that the holding company is not the most important unit of corporate organisation.  The concept of the company does not interest me- it 's the businesses that are important. It does not matter whether or not DRG makes sellotape or Basildon Bond it 's of no consequence to anybody if somebody else makes them.                                            \n",
       "1  Nor could the vaunted irrigation scheme be  described as an unqualified success. The money ran out before the scheme's completion, and the villagers found themselves obliged to borrow the rest. They begged Rytasha for reimbursement, but none was forthcoming, and for the want of a few hundred pounds the village lost its crop for the year.  Now what are we supposed to do? asked one of them.  Are we to sell our cattle or our land?  \n",
       "2  The most irritating thing about Rytasha and there were many was that, though she does not speak their language, she presumes to teach them a spurious form of their own religion.  One thing we do n't need in this subcontinent, Professor Rokeya Kabeer observed, is spiritual guidance we have gurus a dime a dozen.                                                                                                                       \n",
       "3  They are far too civilised for that sort of thing in Bangladesh, but I kept thinking wistfully of T. S. Eliot's lines in Sweeney Agonistes about a nice little white little missionary stew.                                                                                                                                                                                                                                                       \n",
       "4  AT LEAST last Friday's post-election stock market surge was not a one-day special offer for the professionals only. Prices have remained the FT-SE index has risen another 55 points since then allowing even the most passive of private investors, including unit trust holders, to take advantage of the market.                                                                                                                                 "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_chains = pd.read_csv('data/train/longest_chains.csv')  # Paragraphs with longest chains != ''\n",
    "\n",
    "longest_chain_in_paragraphs = longest_chains.merge(paragraphs, left_on='paragraph_id', right_index=True)\n",
    "\n",
    "# We should have a row for each paragraph with a non-empty longest_chain.\n",
    "display(len(paragraphs))\n",
    "display(sum(longest_chains['paragraph_id'].isin(paragraphs.index)))\n",
    "display(len(longest_chain_in_paragraphs['paragraph_id'].drop_duplicates()))\n",
    "display(len(longest_chain_in_paragraphs))  \n",
    "display(len(longest_chains))\n",
    "\n",
    "longest_chain_in_paragraphs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's put our code into reuseable and automated functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/train/academic/words.csv'.format(genre),\n",
    "                      encoding='ISO--8859-1', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_paragraphs(dataset):\n",
    "    all_sentences = pd.read_csv('data/raw/sentences.csv', encoding='ISO-8859-1')\n",
    "\n",
    "    # Sentences the given dataset contains.\n",
    "    sentences = all_sentences.merge(dataset)[['sentence_id', 'paragraph_id', 'sentence']].drop_duplicates()\n",
    "\n",
    "    paragraph_sentences = sentences[sentences['paragraph_id'] != -1]\n",
    "\n",
    "    paragraphs = paragraph_sentences.groupby('paragraph_id')['sentence'].agg(list).apply(lambda x: ' '.join(x))\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def save_paragraphs(paragraphs):\n",
    "    try:\n",
    "        files = glob.glob('data/tmp/paragraphs/*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "    except:\n",
    "        os.mkdir('data/tmp/paragraphs')\n",
    "        \n",
    "    for i, paragraph in paragraphs.iteritems():\n",
    "        file_name = \"data/tmp/paragraphs/paragraph_{}.txt\".format(i)\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            print('Writing {}'.format(file_name))\n",
    "            f.write(paragraph)  \n",
    "\n",
    "\n",
    "def get_longest_chains(genre):\n",
    "    # Go to ELKB directory from the parent directory of this file.\n",
    "#     dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#     os.chdir(os.path.join(dir, '../ELKB'))\n",
    "\n",
    "    with open('data/tmp/longest_chains.csv'.format(genre), 'w', encoding='utf-8') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',')\n",
    "        csv_writer.writerow(('paragraph_id', 'longest_chain'))\n",
    "\n",
    "        for file_name in glob.glob('data/tmp/paragraphs/paragraph*.txt'.format(genre)):\n",
    "            paragraph_id = os.path.splitext(os.path.basename(file_name))[\n",
    "                0].split('_')[-1]\n",
    "\n",
    "            with open(file_name, 'r', encoding='utf-8') as g:\n",
    "                result = subprocess.run(\n",
    "                    'java applications/LexicalChain -f ../{}'.format(file_name),\n",
    "                    cwd='ELKB/',\n",
    "                    stdout=subprocess.PIPE)\n",
    "\n",
    "                result = result.stdout.decode('utf-8')\n",
    "\n",
    "                result = result.strip().split('\\n')\n",
    "\n",
    "                # Does not contain a lexical chain.\n",
    "                # First 3 lines are descriptives by the script.\n",
    "                if len(result) < 4:\n",
    "                    continue\n",
    "\n",
    "                longest_chain = []\n",
    "                len_longest_chain = 0\n",
    "                for r in result[3:]:\n",
    "                    chain = [word.strip()\n",
    "                             for word in r.split('[')[0].split(',')]\n",
    "                    if len(chain) >= len_longest_chain:\n",
    "                        longest_chain.extend(chain)\n",
    "                        \n",
    "                    len_longest_chain = len(chain)\n",
    "                    \n",
    "                # longest_chain = result[3]\n",
    "                # longest_chain = [word.strip() for word in longest_chain.split('[')[0].split(',')]\n",
    "                longest_chain_str = ' '.join(longest_chain)\n",
    "\n",
    "                # line = '{},{}'.format(paragraph_id, longest_chain)\n",
    "                # f.write(line + '\\n')\n",
    "                row = [paragraph_id, longest_chain_str]\n",
    "#                 print(row)\n",
    "                print('Writing longest chain for {} with {} words!\\n'\n",
    "                      .format(file_name, len(longest_chain)))\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "def get_lc_feature(dataset):\n",
    "    longest_chains = pd.read_csv('tmp/longest_chains.csv')  # Paragraphs with longest chains != ''\n",
    "\n",
    "    longest_chain_in_paragraphs = longest_chains.merge(paragraphs, left_on='paragraph_id', right_index=True)\n",
    "\n",
    "    words_and_longest_chains = (\n",
    "        longest_chain_in_paragraphs\n",
    "        .merge(sentences)\n",
    "        .merge(dataset[['sentence_id', 'word_id', 'word']])\n",
    "    )\n",
    "\n",
    "    words_and_longest_chains['longest_chain_list'] = (\n",
    "        words_and_longest_chains['longest_chain']\n",
    "        .apply(lambda x: set(x.split(' ')))\n",
    "    )\n",
    "\n",
    "    words_and_longest_chains['is_in_longest_chain'] = [\n",
    "        word in chain for (word, chain) in zip(\n",
    "            words_and_longest_chains['word'],\n",
    "            words_and_longest_chains['longest_chain_list']\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return words_and_longest_chains[['word_id', 'is_in_longest_chain']]\n",
    "\n",
    "def add_lc_feature(dataset, genre):\n",
    "    paragraphs = build_paragraphs(dataset)\n",
    "    save_paragraphs(paragraphs)\n",
    "    \n",
    "    get_longest_chains(genre)\n",
    "    \n",
    "    lc_feature = get_lc_feature(dataset)\n",
    "    \n",
    "    dataset_with_lc_feature = dataset.merge(lc_feature, how='left')\n",
    "    dataset_with_lc_feature['is_in_longest_chain'].fillna(False)\n",
    "\n",
    "    return dataset_with_lc_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's try our new model with the added Lexical Chain feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48964"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>word_type</th>\n",
       "      <th>function</th>\n",
       "      <th>seg_type</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text_tag</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25070</td>\n",
       "      <td>HER</td>\n",
       "      <td>she</td>\n",
       "      <td>DPS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25071</td>\n",
       "      <td>DRESS</td>\n",
       "      <td>dress</td>\n",
       "      <td>NN1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25072</td>\n",
       "      <td>HANGS</td>\n",
       "      <td>hang</td>\n",
       "      <td>VVZ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25073</td>\n",
       "      <td>HERE'</td>\n",
       "      <td>here'</td>\n",
       "      <td>NP0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25074</td>\n",
       "      <td>DE-FROCKING</td>\n",
       "      <td>de-frock</td>\n",
       "      <td>VVG</td>\n",
       "      <td>mrw</td>\n",
       "      <td>met</td>\n",
       "      <td>1267</td>\n",
       "      <td>a6u-fragment02</td>\n",
       "      <td>a6u</td>\n",
       "      <td>academic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id         word     lemma word_type function seg_type  sentence_id  \\\n",
       "0  25070    HER          she       DPS                         1267          \n",
       "1  25071    DRESS        dress     NN1                         1267          \n",
       "2  25072    HANGS        hang      VVZ                         1267          \n",
       "3  25073    HERE'        here'     NP0                         1267          \n",
       "4  25074    DE-FROCKING  de-frock  VVG       mrw      met      1267          \n",
       "\n",
       "          text_id text_tag     genre  \n",
       "0  a6u-fragment02  a6u      academic  \n",
       "1  a6u-fragment02  a6u      academic  \n",
       "2  a6u-fragment02  a6u      academic  \n",
       "3  a6u-fragment02  a6u      academic  \n",
       "4  a6u-fragment02  a6u      academic  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/train/academic/words.csv', encoding='ISO--8859-1', na_filter=False)\n",
    "display(len(dataset))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Writing data/tmp/paragraphs/paragraph_412.txt\n",
      "Writing data/tmp/paragraphs/paragraph_413.txt\n",
      "Writing data/tmp/paragraphs/paragraph_414.txt\n",
      "Writing data/tmp/paragraphs/paragraph_415.txt\n",
      "Writing data/tmp/paragraphs/paragraph_416.txt\n",
      "Writing data/tmp/paragraphs/paragraph_417.txt\n",
      "Writing data/tmp/paragraphs/paragraph_418.txt\n",
      "Writing data/tmp/paragraphs/paragraph_419.txt\n",
      "Writing data/tmp/paragraphs/paragraph_420.txt\n",
      "Writing data/tmp/paragraphs/paragraph_421.txt\n",
      "Writing data/tmp/paragraphs/paragraph_422.txt\n",
      "Writing data/tmp/paragraphs/paragraph_423.txt\n",
      "Writing data/tmp/paragraphs/paragraph_424.txt\n",
      "Writing data/tmp/paragraphs/paragraph_425.txt\n",
      "Writing data/tmp/paragraphs/paragraph_426.txt\n",
      "Writing data/tmp/paragraphs/paragraph_427.txt\n",
      "Writing data/tmp/paragraphs/paragraph_840.txt\n",
      "Writing data/tmp/paragraphs/paragraph_841.txt\n",
      "Writing data/tmp/paragraphs/paragraph_842.txt\n",
      "Writing data/tmp/paragraphs/paragraph_843.txt\n",
      "Writing data/tmp/paragraphs/paragraph_844.txt\n",
      "Writing data/tmp/paragraphs/paragraph_845.txt\n",
      "Writing data/tmp/paragraphs/paragraph_846.txt\n",
      "Writing data/tmp/paragraphs/paragraph_847.txt\n",
      "Writing data/tmp/paragraphs/paragraph_848.txt\n",
      "Writing data/tmp/paragraphs/paragraph_849.txt\n",
      "Writing data/tmp/paragraphs/paragraph_850.txt\n",
      "Writing data/tmp/paragraphs/paragraph_851.txt\n",
      "Writing data/tmp/paragraphs/paragraph_852.txt\n",
      "Writing data/tmp/paragraphs/paragraph_853.txt\n",
      "Writing data/tmp/paragraphs/paragraph_854.txt\n",
      "Writing data/tmp/paragraphs/paragraph_855.txt\n",
      "Writing data/tmp/paragraphs/paragraph_856.txt\n",
      "Writing data/tmp/paragraphs/paragraph_857.txt\n",
      "Writing data/tmp/paragraphs/paragraph_858.txt\n",
      "Writing data/tmp/paragraphs/paragraph_859.txt\n",
      "Writing data/tmp/paragraphs/paragraph_860.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1043.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1044.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1045.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1046.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1047.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1048.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1049.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1050.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1051.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1052.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1053.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1054.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1055.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1056.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1057.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1058.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1059.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1060.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1061.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1062.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1063.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1064.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1065.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1077.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1078.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1079.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1080.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1081.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1082.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1083.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1084.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1085.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1086.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1087.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1088.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1089.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1090.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1091.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1092.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1093.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1094.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1095.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1096.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1097.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1098.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1099.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1100.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1101.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1102.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1103.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1104.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1105.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1106.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1107.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1108.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1109.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1110.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1111.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1112.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1113.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1114.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1115.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1116.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1117.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1118.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1119.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1120.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1121.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1122.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1123.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1124.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1125.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1126.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1127.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1146.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1147.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1148.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1149.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1150.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1151.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1152.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1153.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1154.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1155.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1156.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1157.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1158.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1159.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1160.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1161.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1162.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1163.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1164.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1165.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1166.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1167.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1168.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1169.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1170.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1171.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1172.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1173.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1174.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1175.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1176.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1177.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1178.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1179.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1180.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1181.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1182.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1183.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1184.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1185.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1186.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1187.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1188.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1189.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1190.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1191.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1192.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1193.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1194.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1195.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1196.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1197.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1198.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1199.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1200.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1201.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1202.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1203.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1204.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1205.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1206.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1207.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1208.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1209.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1210.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1211.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1212.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1213.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1214.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1215.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1216.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1217.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1218.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1219.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/tmp/paragraphs/paragraph_1220.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1221.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1222.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1223.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1224.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1225.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1226.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1227.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1228.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1229.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1230.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1231.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1232.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1233.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1234.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1235.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1236.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1237.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1238.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1239.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1240.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1241.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1242.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1243.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1244.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1245.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1246.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1247.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1248.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1249.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1250.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1251.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1252.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1253.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1254.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1255.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1256.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1257.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1258.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1259.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1260.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1261.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1262.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1263.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1264.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1265.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1266.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1267.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1268.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1269.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1270.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1271.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1272.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1273.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1274.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1275.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1276.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1277.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1278.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1279.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1280.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1281.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1282.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1283.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1284.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1285.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1286.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1287.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1288.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1289.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1290.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1291.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1292.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1293.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1294.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1295.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1296.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1297.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1298.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1299.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1300.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1301.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1302.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1303.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1304.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1305.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1306.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1307.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1308.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1309.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1310.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1311.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1312.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1313.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1314.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1315.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1316.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1317.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1318.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1319.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1320.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1321.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1322.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1323.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1324.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1325.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1326.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1327.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1328.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1329.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1330.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1331.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1332.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1906.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1907.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1908.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1909.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1910.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1911.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1912.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1913.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1914.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1915.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1916.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1917.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1918.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1919.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1920.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1921.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1922.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1923.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1924.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1925.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1926.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1927.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1928.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1929.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1930.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1931.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1932.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1933.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1934.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1935.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1936.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1937.txt\n",
      "Writing data/tmp/paragraphs/paragraph_1938.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2024.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2025.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2026.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2027.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2028.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2029.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2030.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2031.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2032.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2033.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2034.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2035.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2036.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2037.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2038.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2039.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2040.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2041.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2042.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2043.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2044.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2045.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2046.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2047.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2048.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2049.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2050.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2051.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2052.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2053.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2054.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/tmp/paragraphs/paragraph_2055.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2056.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2057.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2058.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2059.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2060.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2061.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2062.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2063.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2064.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2065.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2066.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2067.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2068.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2069.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2070.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2071.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2072.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2073.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2074.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2075.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2076.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2077.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2078.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2079.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2080.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2081.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2082.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2083.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2084.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2085.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2086.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2087.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2088.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2089.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2090.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2091.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2092.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2093.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2094.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2095.txt\n",
      "Writing data/tmp/paragraphs/paragraph_2096.txt\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1043.txt with 6 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1044.txt with 39 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1045.txt with 26 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1046.txt with 24 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1047.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1048.txt with 22 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1049.txt with 21 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1050.txt with 8 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1051.txt with 6 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1052.txt with 28 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1053.txt with 33 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1054.txt with 13 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1055.txt with 10 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1056.txt with 14 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1057.txt with 30 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1058.txt with 14 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1059.txt with 6 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1060.txt with 6 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1061.txt with 8 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1063.txt with 9 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1064.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1065.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1077.txt with 3 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1078.txt with 18 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1079.txt with 27 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1080.txt with 22 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1081.txt with 30 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1082.txt with 7 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1083.txt with 15 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1084.txt with 21 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1085.txt with 6 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1086.txt with 10 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1087.txt with 23 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1088.txt with 25 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1089.txt with 3 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1090.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1091.txt with 4 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1092.txt with 8 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1093.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1094.txt with 19 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1095.txt with 39 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1096.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1097.txt with 29 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1098.txt with 20 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1099.txt with 19 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1100.txt with 74 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1101.txt with 38 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1102.txt with 30 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1103.txt with 16 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1104.txt with 16 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1105.txt with 28 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1106.txt with 26 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1107.txt with 8 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1108.txt with 15 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1109.txt with 23 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1110.txt with 2 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1111.txt with 11 words!\n",
      "\n",
      "Writing longest chain for data/tmp/paragraphs\\paragraph_1112.txt with 11 words!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre = 'academic'\n",
    "train_set = pd.read_csv('data/train/{}/words.csv'.format(genre), encoding='ISO--8859-1', na_filter=False)\n",
    "X_train, test_fold_ids, vocabulary, y_train = preprocess_data(train_set, want_lc=True)\n",
    "\n",
    "unigram_classifier = UnigramClassifier(\n",
    "    verbose=10, random_state=0, C=10, penalty='l1',\n",
    "    solver='liblinear', class_weight='balanced', max_iter=500,\n",
    "    vocabulary=vocabulary\n",
    ")\n",
    "\n",
    "C = np.logspace(-2,4,20)\n",
    "penalty = ['l1', 'l2']\n",
    "hyperparameters = {'C':C, 'penalty':penalty}\n",
    "clf = GridSearchCV(unigram_classifier, hyperparameters,\n",
    "                   cv=PredefinedSplit(test_fold_ids),\n",
    "                   verbose=10, scoring='f1')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test_set = pd.read_csv('data/test/{}/words.csv'.format(genre), encoding='ISO--8859-1', na_filter=False)\n",
    "X_test, _, _, y_test = preprocess_data(test_set)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "result = classification_report(y_test, preds, output_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
